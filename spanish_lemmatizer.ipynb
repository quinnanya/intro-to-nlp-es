{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "spanish-lemmatizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvCakcm7wt4B"
      },
      "source": [
        "# Spanish lemmatizer\n",
        "\n",
        "This Jupyter notebook takes a text file, or folder of text files in Spanish, and creates a set of lemmatized derivative files (where all words are in their dictionary form, and not inflected). These lemmatized files can then be used for searching, or other computational text analysis methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnC4c6q5wt4R"
      },
      "source": [
        "### Mounting Google drive\n",
        "Run the code cell below. You'll get a link that will take you to a screen where you can choose which Google account to authenticate with. After you choose an account and approve the connection, you'll get a long string of numbers and letters. Copy and paste it into the box that will appear in the code cell below, and hit enter.\n",
        "\n",
        "If you see `Mounted at /content/gdrive`, then you know it worked."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMiVRnfGxDVY"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJgA23Bmwt4C"
      },
      "source": [
        "## Setup\n",
        "The code cells below below install the *spacy* package which can do the actual lemmatizing. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3zFALK8wt4C"
      },
      "source": [
        "#Imports the module you need to download and install the spaCy modules\n",
        "import sys\n",
        "#Installs spaCy\n",
        "!{sys.executable} -m pip install spacy==3.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brucb21ZxsQj"
      },
      "source": [
        "#Replace es_core_news_lg with another model name here for other languages\n",
        "import spacy\n",
        "!{sys.executable} -m spacy download es_core_news_lg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65X6aMjWwt4H"
      },
      "source": [
        "### Import modules\n",
        "To use a [SpaCy language model](https://spacy.io/models) other than Spanish, replace `es_core_news_lg` with the model name in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSOWng7pwt4I"
      },
      "source": [
        "# os is used for navigating directories\n",
        "import os\n",
        "# spacy is used for identifying the subjects and verbs\n",
        "import spacy\n",
        "#Replace en_core_web_sm with another model name here for other languages\n",
        "import es_core_news_lg\n",
        "#Replace en_core_web_sm with another model name here for other languages\n",
        "nlp = spacy.load(\"es_core_news_lg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQkuDsERlefq"
      },
      "source": [
        "## Lemmatizing a single file\n",
        "\n",
        "\n",
        "**Skip down to the next section if you want to lemmatize a whole folder**\n",
        "\n",
        "After uploading the text file to Drive, put in the full path to where it's located. All paths should begin with \\'/content/gdrive/My Drive/'. \n",
        "\n",
        "If your file is located just in the root of your Drive (not in a subfolder), the path should look like '/content/gdrive/My Drive/your-text-file.txt'\n",
        "\n",
        "If it's in a subfolder, it should look like: '/content/gdrive/My Drive/subfolder/your-text-file.txt' (you can also include multiple subfolders if it's nested)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHzEoPcPE3Dt"
      },
      "source": [
        "#Put the full path to your file between the single quotes here\n",
        "filepath = '/content/gdrive/MyDrive/intro-to-nlp-es-files/fortunata_y_jacinta_1.txt'\n",
        "\n",
        "#The outname is the name of the lemmatized file that this notebook creates\n",
        "#If you want it to be named something other than the original file name + -lemmatized\n",
        "#you can change that here\n",
        "outname = filepath.replace('.txt', '-lemmatized.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5xEDEgIwt4V"
      },
      "source": [
        "#Opens the file you specified\n",
        "with open(filepath, 'r', encoding='utf8') as f:\n",
        "    #Creates an empty text file with -lemmatized.txt appended to the name\n",
        "    with open(outname, 'w', encoding='utf8') as out:\n",
        "        #Reads the text of the file you specified\n",
        "        text = f.read()\n",
        "        #Does Spanish NLP on the text\n",
        "        doc = nlp(text)\n",
        "        #For each word in the text...\n",
        "        for token in doc:\n",
        "            #Write the lemma to the new text file with the lemmatized text\n",
        "            out.write(token.lemma_)\n",
        "            #Write a space after each word\n",
        "            out.write(' ')\n",
        "            #Print the lemmas to the screen below, with a space between them\n",
        "            print(token.lemma_, end=' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAFahAiKjDoc"
      },
      "source": [
        "## Lemmatizing an entire folder of text files\n",
        "If want to lemmatize an entire folder of files, put the path to the folder here. *Please include only paths to folders that contain text (.txt) files. If you have a folder that contains sub-folders of text files.*\n",
        "\n",
        "Paths should look like: '/content/gdrive/My Drive/subfolder' (you can also include multiple subfolders if it's nested)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0OCq6-i3HN1"
      },
      "source": [
        "#Put the full path to your folder between single quotes here\n",
        "textfolder = '/content/gdrive/My Drive/intro-to-nlp-es-files'\n",
        "#Changes the working directory to the folder you specified\n",
        "os.chdir(textfolder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPO6c4WbjsXM"
      },
      "source": [
        "#For every file in the folder you specified...\n",
        "for filename in os.listdir(textfolder):\n",
        "    #If it's a text file, but not one of the text files with just lemmas\n",
        "    if filename.endswith('.txt') and not filename.endswith('-lemmatized.txt'):\n",
        "        #The outname is the name of the lemmatized file that this notebook creates\n",
        "        #If you want it to be named something other than the original file name + -lemmatized\n",
        "        #you can change that here\n",
        "        outname = filename.replace('.txt', '-lemmatized.txt')\n",
        "        #Opens the file you specified\n",
        "        with open(filename, 'r', encoding='utf8') as f:\n",
        "            #Creates an empty text file with -lemmatized.txt appended to the name\n",
        "            with open(outname, 'w', encoding='utf8') as out:\n",
        "                #Reads the text of the file you specified\n",
        "                text = f.read()\n",
        "                #Removes any problematic punctuation\n",
        "                #Does Spanish NLP on the cleaned text\n",
        "                doc = nlp(text)\n",
        "                #For each word in the text...\n",
        "                for token in doc:\n",
        "                    #Write the lemma to the new text file with the lemmatized text\n",
        "                    out.write(token.lemma_)\n",
        "                    #Write a space after each word\n",
        "                    out.write(' ')\n",
        "                    #Print the lemmas to the screen below, with a space between them\n",
        "                    #print(token.lemma_, end=' ')\n",
        "print('Your folder now has lemmatized files!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeDl2jHlwt4Y"
      },
      "source": [
        "## About\n",
        "\n",
        "This Jupyter notebook was originally developed by Quinn Dombrowski for use in [DLCL 204: Digital Humanities Across Borders](https://github.com/quinnanya/dlcl204) at Stanford University, fall 2020. "
      ]
    }
  ]
}