{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "spanish-nlp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvCakcm7wt4B"
      },
      "source": [
        "# Intro to NLP (Spanish)\n",
        "\n",
        "This Jupyter notebook lets you explore the different ways the spaCy Python library can annotate Spanish-language text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnC4c6q5wt4R"
      },
      "source": [
        "### Mounting Google drive\n",
        "Run the code cell below. You'll get a link that will take you to a screen where you can choose which Google account to authenticate with. After you choose an account and approve the connection, you'll get a long string of numbers and letters. Copy and paste it into the box that will appear in the code cell below, and hit enter.\n",
        "\n",
        "If you see `Mounted at /content/gdrive`, then you know it worked."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMiVRnfGxDVY"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJgA23Bmwt4C"
      },
      "source": [
        "## Setup\n",
        "The code cells below below install the *spacy* package which we will use for NLP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3zFALK8wt4C"
      },
      "source": [
        "#Imports the module you need to download and install the spaCy modules\n",
        "import sys\n",
        "#Installs spaCy\n",
        "!{sys.executable} -m pip install spacy==3.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brucb21ZxsQj"
      },
      "source": [
        "#Replace es_core_news_lg with another model name here for other languages\n",
        "import spacy\n",
        "!{sys.executable} -m spacy download es_core_news_lg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65X6aMjWwt4H"
      },
      "source": [
        "### Import modules\n",
        "To use a [SpaCy language model](https://spacy.io/models) other than Spanish, replace `es_core_news_lg` with the model name in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSOWng7pwt4I"
      },
      "source": [
        "# os is used for navigating directories\n",
        "import os\n",
        "# spacy is used for identifying the subjects and verbs\n",
        "import spacy\n",
        "from spacy.symbols import nsubj, VERB\n",
        "#Replace core_news_lg with another model name here for other languages\n",
        "import es_core_news_lg\n",
        "#Replace en_core_web_sm with another model name here for other languages\n",
        "nlp = spacy.load(\"es_core_news_lg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQkuDsERlefq"
      },
      "source": [
        "## Exploring spaCy tagging\n",
        "\n",
        "Before you run spaCy on a whole text, try it on a few sentences in order to understand what the different annotations are and how they work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9ieg9PY3inp"
      },
      "source": [
        "example = nlp(\"Empezó Maximiliano sus estudios el 69, y su hermano y su tía le ponderaban lo bonita que era la Farmacia y lo mucho que con ella se ganaba, por ser muy caros los medicamentos y muy baratas las primeras materias: agua del pozo, ceniza del fogón, tierra de los tiestos, etcétera... El pobre chico, que era muy dócil, con todo se mostraba conforme.\")\n",
        "\n",
        "for token in example:\n",
        "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
        "            token.shape_, token.is_alpha, token.is_stop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuOyCyPJ4ZJK"
      },
      "source": [
        "from spacy import displacy\n",
        "displacy.render(example, style='dep', jupyter=True, options={'distance': 90})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otrtciV75b-0"
      },
      "source": [
        "## Getting all nouns\n",
        "When comparing texts based on their content, it can be useful to create derivative files with only the words that give us the best information about the content of the text. Nouns usually convey the most information about what a text is about. The code cells below take each of the text files in the *intro-to-nlp-es-files* folder in your Google Drive, and creates a derivative that only has the nouns from the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHzEoPcPE3Dt"
      },
      "source": [
        "#Put the full path to your folder between single quotes here\n",
        "textfolder = '/content/gdrive/My Drive/intro-to-nlp-es-files'\n",
        "#Changes the working directory to the folder you specified\n",
        "os.chdir(textfolder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5xEDEgIwt4V"
      },
      "source": [
        "#For every file in the folder you specified...\n",
        "for filename in os.listdir(textfolder):\n",
        "    #If it's a text file, but not one of the text files with just lemmas\n",
        "    if filename.endswith('.txt') and not filename.endswith('-lemmatized.txt'):\n",
        "        #The outname is the name of the nouns-only file that this notebook creates\n",
        "        #If you want it to be named something other than the original file name + -nouns\n",
        "        #you can change that here\n",
        "        outname = filename.replace('.txt', '-nouns.txt')\n",
        "        #Opens the file you specified\n",
        "        with open(filename, 'r', encoding='utf8') as f:\n",
        "            #Creates an empty text file with -nouns.txt appended to the name\n",
        "            with open(outname, 'w', encoding='utf8') as out:\n",
        "                #Reads the text of the file you specified\n",
        "                text = f.read()\n",
        "                #Removes any problematic punctuation\n",
        "                #Does Spanish NLP on the cleaned text\n",
        "                doc = nlp(text)\n",
        "                #For each word in the text...\n",
        "                for token in doc:\n",
        "                    if token.pos_ == 'NOUN':\n",
        "                      #Write the lemma to the new text file with the lemmatized text\n",
        "                      out.write(token.lemma_)\n",
        "                      #Write a space after each word\n",
        "                      out.write(' ')\n",
        "print('Your folder now has files with only nouns!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAFahAiKjDoc"
      },
      "source": [
        "## Extracting character verbs\n",
        "You can also use spaCy's dependency parse to try to identify all of a character's verbs: what are different characters *doing* in the text?\n",
        "\n",
        "Ideally, there'd be another step in the pipeline that performs *co-reference resolution* (figuring out which character all the 'he', 'she', 'I', etc. are referring to), but that is still a very hard computational problem. (The only relatively easy-to-use tool that does it for English, somewhat successfully, is David Bamman's [BookNLP](https://github.com/dbamman/book-nlp)).\n",
        "\n",
        "Here, we've defined a list of major characters in *Fortunata and Jacinta*. The code below looks for all places where the subject of a sentence (nsubj) matches one of those character names, and writes out that character and the main verb from the sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6gfS-mKAqC4"
      },
      "source": [
        "names = ['Fortunata', 'Juanito', 'Juanito Santa Cruz', 'Jacinta', 'Maximiliano', 'Maxi', 'Juárez', 'el Negro', 'Juan Evaristo', 'D. Evaristo', 'D. José', 'José Izquierdo', 'Mauricia', 'Aurora']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpiDVvL5pbzB"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHT3HgxdAt3G"
      },
      "source": [
        "#You can name your output file here something else if you like\n",
        "#This file will appear in the same folder in Drive as the text files\n",
        "charverbfile = 'fortunata-jacinda-verbs2.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0OCq6-i3HN1"
      },
      "source": [
        "#Put the full path to your folder between single quotes here\n",
        "textfolder = '/content/gdrive/My Drive/intro-to-nlp-es-files'\n",
        "#Changes the working directory to the folder you specified\n",
        "os.chdir(textfolder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPO6c4WbjsXM"
      },
      "source": [
        "#For every file in the folder you specified...\n",
        "with open(charverbfile, 'w') as out:\n",
        "  for filename in os.listdir(textfolder):\n",
        "      #If it's a text file, but not one of the text files with just lemmas or nouns\n",
        "      if filename.endswith('.txt') and not filename.endswith('-lemmatized.txt') and not filename.endswith('-nouns.txt'):\n",
        "          #The outname is the name of the lemmatized file that this notebook creates\n",
        "          #If you want it to be named something other than the original file name + -lemmatized\n",
        "          #you can change that here\n",
        "          #Opens each file\n",
        "          with open(filename, 'r') as bookfile:\n",
        "              #Reads in the text in the file\n",
        "              book = bookfile.read()\n",
        "              #NLP parse of the text\n",
        "              doc = nlp(book)\n",
        "              for possible_subject in doc:\n",
        "                for name in names:\n",
        "                  if possible_subject.text == name:\n",
        "                    if possible_subject.dep == nsubj and possible_subject.head.pos == VERB:\n",
        "                      out.write(str(possible_subject) + ', ')\n",
        "                      out.write(str(possible_subject.head) + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fY7KBO_0mBWa"
      },
      "source": [
        "## Lemmatizing character verbs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCVLyNYVb47Z"
      },
      "source": [
        "#Import CSV reader\n",
        "import csv\n",
        "#Open a new file that will include a column for the lemmatized verbs\n",
        "with open('verbs-lemmatized.csv', 'w') as out:\n",
        "  #Writes header row\n",
        "  out.write('Character, Verb, VerbLemma\\n')\n",
        "  #Opens the character verb file\n",
        "  with open('fortunata-jacinda-verbs2.csv', 'r') as csvfile:\n",
        "    #Reads the character verb file\n",
        "    csvreader = csv.reader(csvfile, delimiter=',')\n",
        "    #For each row...\n",
        "    for row in csvreader:\n",
        "      #Character is first column\n",
        "      character = row[0]\n",
        "      #Verb is second column\n",
        "      verb = row[1]\n",
        "      #NLP on verb\n",
        "      analyzed = nlp(verb)\n",
        "      #For word in analyzed text\n",
        "      for token in analyzed:\n",
        "        #Skip the first one (there are two, the first is blank, not sure why)\n",
        "        if token.i == 1:\n",
        "          #Write out the result\n",
        "          out.write(character + ', ' + verb + ', ' + token.lemma_ + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}