{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0. Download SpaCy and language model\n",
    "If you don't already have the SpaCy library installed, the code below downloads it along with the English language model. This notebook should work for any [language with a SpaCy model](https://spacy.io/models). Just substitute the name of the model for *en_core_web_sm* (For instance, if you wanted to use Lithuanian, you can replace `en_core_web_sm` with `lt_core_news_sm`). Places where you need to do this substitution are commented in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace es_core_news_lg with another model name here for other languages\n",
    "!python -m spacy download es_core_news_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Import modules\n",
    "To use a [SpaCy language model](https://spacy.io/models) other than English, replace `en_core_web_sm` with the model name in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os is used for navigating directories\n",
    "import os\n",
    "# spacy is used for identifying the subjects and verbs\n",
    "import spacy\n",
    "#Replace en_core_web_sm with another model name here for other languages\n",
    "import es_core_news_lg\n",
    "#Replace en_core_web_sm with another model name here for other languages\n",
    "nlp = spacy.load(\"es_core_news_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring spaCy tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = nlp(\"Empezó Maximiliano sus estudios el 69, y su hermano y su tía le ponderaban lo bonita que era la Farmacia y lo mucho que con ella se ganaba, por ser muy caros los medicamentos y muy baratas las primeras materias: agua del pozo, ceniza del fogón, tierra de los tiestos, etcétera... El pobre chico, que era muy dócil, con todo se mostraba conforme.\")\n",
    "\n",
    "for token in example:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(example, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Find subjects\n",
    "The code cells below reads each text file in the directory, finds every subject and verb, and writes it to a CSV file along with the filename it's from. The files are processed alphabetically, so that if something breaks (e.g. text files bigger than 1 MB may trigger a memory error), you can figure out what files are left to be done.\n",
    "\n",
    "By default, the CSV file is called `charverbs.csv` and gets created inside the directory with the text files. You can give the file a different name in the code cell below, but keeping `.csv` is recommended.\n",
    "\n",
    "Doing the NLP parse can be time-consuming, particularly if you have a large number of files. If your text is on the scale of hundreds of novels, expect it to take hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charverbfile = 'fortunata-jacinda-verbs.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opens the output file\n",
    "with open(charverbfile, 'w') as out:\n",
    "    out.write('Filename, Subject, Verb' + '\\n')\n",
    "    #Sorts the files alphabetically\n",
    "    for filename in sorted(os.listdir('.')):\n",
    "        #Looks for .txt files\n",
    "        if filename.endswith('.txt'):\n",
    "            #Opens each file\n",
    "            with open(filename, 'r') as bookfile:\n",
    "                #Reads in the text in the file\n",
    "                book = bookfile.read()\n",
    "                #NLP parse of the text\n",
    "                doc = nlp(book)\n",
    "                #Noun chunks are the part of the SpaCy dependency parse that we need\n",
    "                for chunk in doc.noun_chunks:\n",
    "                    #If the dependency relation is 'nsubj' (noun subject)\n",
    "                    if chunk.root.dep_ == 'nsubj':\n",
    "                        #Write the filename, the noun chunk, the verb, and then a newline character\n",
    "                        strsubj = str(chunk.text)\n",
    "                        cleansubj = strsubj.replace(',', '')                        \n",
    "                        out.write(filename + ', ' + cleansubj + ', ' + chunk.root.head.text + '\\n')\n",
    "                        print(filename + ', ' + cleansubj + ', ' + chunk.root.head.text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
